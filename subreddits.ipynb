{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "import re\n",
    "import pdb\n",
    "from collections import defaultdict, Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/dfs/scratch0/wleif/Reddit/clean_comments/'\n",
    "subreddits = os.listdir(path)\n",
    "subreddits = ['funny', 'leagueoflegends', 'AdviceAnimals', 'pics', 'nfl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -n1 -r1\n",
    "print_every = 1\n",
    "\n",
    "usr2ind = {}\n",
    "ind2usr = {}\n",
    "n_users = 0\n",
    "\n",
    "sub2ind = {}\n",
    "ind2sub = {}\n",
    "n_subs = len(subreddits)\n",
    "\n",
    "users = {}\n",
    "\n",
    "'''\n",
    "sample = np.random.random_integers(0, len(subreddits), nsubs)\n",
    "print sampled_subreddits\n",
    "for i, sub_idx in enumerate(sample):\n",
    "    subreddit = subreddits[sub_idx]\n",
    "    sub2ind[subreddit] = sub_idx\n",
    "    ind2sub[sub_idx] = subreddit\n",
    "'''\n",
    "\n",
    "for i, subreddit in enumerate(subreddits):\n",
    "    sub2ind[subreddit] = i\n",
    "    ind2sub[i] = subreddit\n",
    "    with open(path+subreddit+'.tsv', 'r') as df:\n",
    "        #raw_data = np.genfromtxt(df, dtype=str, delimiter='\\t')\n",
    "        for row in df:\n",
    "            user = row.split('\\t')[-5]\n",
    "            if user not in usr2ind:\n",
    "                ind2usr[n_users] = user\n",
    "                usr2ind[user] = n_users\n",
    "                users[n_users] = defaultdict(int)\n",
    "                n_users += 1\n",
    "            users[usr2ind[user]][i] += 1\n",
    "\n",
    "    \n",
    "    if i % print_every == 0:\n",
    "        print \"Finished %d\" % (i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsubs: 0, percent: 0.302, robust count: 586041, full count: 1939091\n",
      "nsubs: 1, percent: 0.125, robust count: 242105, full count: 898131\n",
      "nsubs: 2, percent: 0.059, robust count: 114459, full count: 433442\n",
      "nsubs: 3, percent: 0.007, robust count: 13687, full count: 67131\n",
      "nsubs: 4, percent: 0.000, robust count: 505, full count: 3684\n",
      "Total number of users: 1939091, Number of selected users: 13687\n",
      "Total number of posts: 40737131\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Some analytics\n",
    "\n",
    "'''\n",
    "\n",
    "counter = [0.0] * n_subs\n",
    "r_counter = [0.0] * n_subs\n",
    "n_posts = 0\n",
    "thresh = 3\n",
    "user_pool = []\n",
    "\n",
    "def robust_count(d, n_spans = 5):\n",
    "    return sum([1 if v > n_spans else 0 for v in [v for v in d.values()] ])\n",
    "        \n",
    "for k, v in users.iteritems():\n",
    "    n_posts += sum([uv for _, uv in v.iteritems()])\n",
    "    for i in xrange(n_subs):\n",
    "        if len(v) > i:\n",
    "            counter[i] += 1\n",
    "        if robust_count(v) > i:\n",
    "            r_counter[i] += 1\n",
    "    if robust_count(v) > thresh:\n",
    "        user_pool.append(k)\n",
    "percents = [n / n_users for n in r_counter]\n",
    "for i, percent in enumerate(percents):\n",
    "    print \"nsubs: %d, percent: %.3f, robust count: %d, full count: %d\" % (i, percent, r_counter[i], counter[i])\n",
    "\n",
    "user_pool = set(user_pool)\n",
    "print \"Total number of users: %d, Number of selected users: %d\" % (n_users, len(user_pool))\n",
    "print \"Total number of posts: %d\" % n_posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0.00\n",
      "\t0.20\n",
      "\t0.40\n",
      "\t0.60\n",
      "\t0.80\n",
      "\t1.00\n",
      "Finished 1\n",
      "\t0.00\n",
      "\t0.20\n",
      "\t0.40\n",
      "\t0.60\n",
      "\t0.80\n",
      "Finished 2\n",
      "\t0.00\n",
      "\t0.20\n",
      "\t0.40\n",
      "\t0.60\n",
      "\t0.80\n",
      "\t1.00\n",
      "Finished 3\n",
      "\t0.00\n",
      "\t0.20\n",
      "\t0.40\n",
      "\t0.60\n",
      "\t0.80\n",
      "\t1.00\n",
      "Finished 4\n",
      "\t0.00\n",
      "\t0.20\n",
      "\t0.40\n",
      "\t0.60\n",
      "\t0.80\n",
      "\t1.00\n",
      "Finished 5\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Get the posts for the users that selected population of users (user_pool)\n",
    "\n",
    "'''\n",
    "\n",
    "print_every = [1.,1.]\n",
    "posts = [[[] for x in xrange(n_users)] for y in xrange(n_subs)]\n",
    "\n",
    "try:\n",
    "    for i, subreddit in enumerate(subreddits):\n",
    "        nrows = sum(1 for row in open(path+subreddit+'.tsv', 'r'))\n",
    "        with open(path+subreddit+'.tsv', 'r') as df:\n",
    "            sub_ind = sub2ind[subreddit]\n",
    "            for j, row in enumerate(df):\n",
    "                data = row.split('\\t')\n",
    "                user = usr2ind[data[-5]]\n",
    "                if user in user_pool:\n",
    "                    posts[sub_ind][user].append((data[-1], data[1]))\n",
    "\n",
    "                if j % int(nrows/print_every[0]) == 0: ## TODO: Fix logging...\n",
    "                    print '\\t%.2f' % (j/float(nrows))\n",
    "\n",
    "        if i % print_every[1] == 0:\n",
    "            print \"Finished %d\" % (i+1)\n",
    "except Exception as e:\n",
    "    pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print [0 for x in xrange(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting by date and gathering vocab...\n",
      "Pruning vocab...\n",
      "Writing vocab...\n",
      "\tFull vocab size: 519523, pruned vocab size: 10000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Build, prune, and write vocab\n",
    "\n",
    "'''\n",
    "\n",
    "min_doc_appearances = 4\n",
    "remove_top_k = 500\n",
    "max_vocab_size = 10000\n",
    "outfile = '/dfs/scratch0/wangalex/rmn/reddit5'\n",
    "\n",
    "freqs = defaultdict(int)\n",
    "doc_freqs = {}\n",
    "special = ['<EOS>', '<UNK>', '<SPECIAL>', '<URL>']\n",
    "lens = []\n",
    "max_len = 0\n",
    "\n",
    "print \"Sorting by date and gathering vocab...\"\n",
    "for i in xrange(n_subs):\n",
    "    for j in xrange(n_users):\n",
    "        if posts[i][j] is []:\n",
    "            continue\n",
    "        posts[i][j].sort(key=lambda tup:tup[-1])\n",
    "        for post,_ in posts[i][j]:\n",
    "            clean_words = re.sub(r\"[0-9]{1,}\", \"<NUM>\", re.sub(r\"<(.+?)>\", \"\", post)).strip().split()\n",
    "            for word in clean_words:\n",
    "                if word not in doc_freqs:\n",
    "                    doc_freqs[word] = [0 for x in xrange(nsubs)]\n",
    "                doc_freqs[word][i] = 1\n",
    "                freqs[word] += 1\n",
    "            lens.append(len(clean_words))\n",
    "            max_len = max(max_len, len(clean_words))\n",
    "\n",
    "# pruning vocab borrowed from Yoon Kim\n",
    "print 'Pruning vocab...'\n",
    "vocab = [(word, count) for word, count in freqs.iteritems()]\n",
    "vocab.sort(key = lambda x: x[1], reverse = True)\n",
    "doc_freqs = {word:sum(freq) for word, freq in doc_freqs.iteritems()}\n",
    "no_common_words = [pair[0] for pair in vocab[remove_top_k:]]\n",
    "min_doc_words = filter(lambda x: doc_freqs[x] >= min_doc_appearances, no_common_words)\n",
    "pruned = min_doc_words[:max_vocab_size-len(special)]\n",
    "\n",
    "word2ind = {}\n",
    "ind2word = {}\n",
    "ind = 0\n",
    "for word in special+pruned:\n",
    "    word2ind[word] = ind\n",
    "    ind2word[ind] = word\n",
    "    ind += 1\n",
    "\n",
    "print \"Writing vocab...\"\n",
    "with open(outfile+'.vocab.txt', 'w') as f:\n",
    "    f.write(\"Word Index Count DocFreq\\n\")\n",
    "    words = [(word, idx) for word, idx in word2ind.iteritems()]\n",
    "    words.sort(key = lambda x: x[1])\n",
    "    for word, idx in words:\n",
    "        if word in freqs:\n",
    "            f.write(\"%s %d %d %d\\n\" % (word, idx, freqs[word], doc_freqs[word]))\n",
    "        else:\n",
    "            f.write(\"%s %d %d %d\\n\" % (word, idx, -1, -1))\n",
    "\n",
    "with open(outfile+'.vocab.pkl', 'w') as f:\n",
    "    pickle.dump((word2ind, ind2word), f)\n",
    "\n",
    "print '\\tFull vocab size: %d, pruned vocab size: %d' % (len(vocab), len(word2ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Build word2vec pretrained embeddings\n",
    "\n",
    "'''\n",
    "\n",
    "word_vecs = {}\n",
    "vec_file = '/dfs/scratch0/wangalex/GoogleNews-vectors-negative300.bin.gz'\n",
    "with open(vec_file, \"rb\") as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, layer1_size = map(int, header.split())\n",
    "    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "    for line in xrange(vocab_size):\n",
    "        word = []\n",
    "        while True:\n",
    "            ch = f.read(1)\n",
    "            if ch == ' ':\n",
    "                word = ''.join(word)\n",
    "                break\n",
    "            if ch != '\\n':\n",
    "                word.append(ch)\n",
    "        if word in vocab:  \n",
    "            word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "        else:\n",
    "            f.read(binary_len)\n",
    "\n",
    "embed = np.random.uniform(-0.25, 0.25, (len(vocab), len(word_vecs.values()[0])))\n",
    "embed[0] = 0\n",
    "for word, vec in word_vecs.items():\n",
    "    embed[vocab[word]] = vec\n",
    "print \"\\tLoaded %d vectors\" % len(word_vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts = []\n",
    "\n",
    "for i in xrange(n_subs):\n",
    "    sub = np.array([i], dtype=np.int32)\n",
    "    for j in xrange(n_users):\n",
    "        if posts[i][j] is []:\n",
    "            continue\n",
    "        user = np.array([j], dtype=np.int32)\n",
    "        spans = []\n",
    "        masks = []\n",
    "        for post,_ in posts[i][j]:\n",
    "            clean_words = re.sub(r\"[0-9]{1,}\", \"<NUM>\", re.sub(r\"<(.+?)>\", \"\", post)).strip().split()\n",
    "            span = [word2ind[word] if word in word2ind else word2ind['UNK'] for word in clean_words[:max_len]] + /\n",
    "                    [-1]*(max_len-len(clean_words))\n",
    "            mask = (span >= 0).astype(int)\n",
    "            spans.append(span)\n",
    "            masks.append(mask)\n",
    "    posts.append([sub, user, np.array(spans, dtype=np.int32), np.array(mask, dtype=np.int32)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
