{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "import re\n",
    "import pdb\n",
    "from collections import defaultdict, Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/dfs/scratch0/wleif/Reddit/clean_comments/'\n",
    "subreddits = os.listdir(path)\n",
    "subreddits = ['atheism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -n1 -r1\n",
    "print_every = 1\n",
    "\n",
    "usr2ind = {}\n",
    "ind2usr = {}\n",
    "n_users = 0\n",
    "\n",
    "sub2ind = {}\n",
    "ind2sub = {}\n",
    "n_subs = len(subreddits)\n",
    "\n",
    "users = {}\n",
    "\n",
    "for i, subreddit in enumerate(subreddits):\n",
    "    sub2ind[subreddit] = i\n",
    "    ind2sub[i] = subreddit\n",
    "    with open(path+subreddit+'.tsv', 'r') as df:\n",
    "        #raw_data = np.genfromtxt(df, dtype=str, delimiter='\\t')\n",
    "        for row in df:\n",
    "            user = row.split('\\t')[-5]\n",
    "            if user not in usr2ind:\n",
    "                ind2usr[n_users] = user\n",
    "                usr2ind[user] = n_users\n",
    "                users[n_users] = defaultdict(int)\n",
    "                n_users += 1\n",
    "            users[usr2ind[user]][i] += 1\n",
    "\n",
    "    \n",
    "    if i % print_every == 0:\n",
    "        print \"Finished %d\" % (i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsubs: 0, percent: 0.061, robust count: 9006, full count: 146621\n",
      "Total number of users: 146621, Number of selected users: 9006\n",
      "Total number of posts: 1535363\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Some analytics\n",
    "\n",
    "'''\n",
    "\n",
    "counter = [0.0] * n_subs\n",
    "r_counter = [0.0] * n_subs\n",
    "n_posts = 0\n",
    "thresh_sub = 0\n",
    "thresh_posts = 25\n",
    "user_pool = []\n",
    "\n",
    "# counts the number of subreddits for which the user has posted > min_span times\n",
    "def robust_count(d, min_spans = 5):\n",
    "    return sum([1 if v >= min_spans else 0 for v in [v for v in d.values()] ])\n",
    "        \n",
    "for k, v in users.iteritems():\n",
    "    n_posts += sum([uv for _, uv in v.iteritems()]) # count number of posts user made total\n",
    "    for i in xrange(len(v)):\n",
    "        counter[i] += 1\n",
    "    for i in xrange(robust_count(v, thresh_posts)):\n",
    "        r_counter[i] += 1\n",
    "    if robust_count(v, thresh_posts) > thresh_sub:\n",
    "        user_pool.append(k)\n",
    "percents = [n / n_users for n in r_counter]\n",
    "for i, percent in enumerate(percents):\n",
    "    print \"nsubs: %d, percent: %.3f, robust count: %d, full count: %d\" % (i, percent, r_counter[i], counter[i])\n",
    "\n",
    "user_pool = set(user_pool)\n",
    "print \"Total number of users: %d, Number of selected users: %d\" % (n_users, len(user_pool))\n",
    "print \"Total number of posts: %d\" % n_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0.00\n",
      "Finished 1\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Get the posts for the users that selected population of users (user_pool)\n",
    "\n",
    "'''\n",
    "\n",
    "print_every = [4.,1.]\n",
    "posts = [[[] for x in xrange(n_users)] for y in xrange(n_subs)]\n",
    "\n",
    "for i, subreddit in enumerate(subreddits):\n",
    "    nrows = sum(1 for row in open(path+subreddit+'.tsv', 'r'))\n",
    "    with open(path+subreddit+'.tsv', 'r') as df:\n",
    "        sub_ind = sub2ind[subreddit]\n",
    "        for j, row in enumerate(df):\n",
    "            data = row.split('\\t')\n",
    "            user = usr2ind[data[-5]]\n",
    "            if user in user_pool:\n",
    "                posts[sub_ind][user].append((data[-1], data[1]))\n",
    "\n",
    "            if j % int(nrows/print_every[0]) == 0:\n",
    "                print '\\t%.2f' % (j/float(nrows))\n",
    "\n",
    "    if i % print_every[1] == 0:\n",
    "        print \"Finished %d\" % (i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting by date and gathering vocab...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Build vocab\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "outfile = '/dfs/scratch0/wangalex/rmn/reddit5'\n",
    "\n",
    "freqs = defaultdict(int)\n",
    "doc_freqs = {}\n",
    "special = ['<EOS>', '<UNK>', '<SPECIAL>', '<URL>']\n",
    "lens = []\n",
    "max_len = 0\n",
    "\n",
    "print \"Sorting by date and gathering vocab...\"\n",
    "for i in xrange(n_subs):\n",
    "    for j in xrange(n_users):\n",
    "        if posts[i][j] is []:\n",
    "            continue\n",
    "        posts[i][j].sort(key=lambda tup:tup[-1])\n",
    "        for post,_ in posts[i][j]:\n",
    "            clean_words = re.sub(r\"[0-9]{1,}\", \"<NUM>\", re.sub(r\"<(.+?)>\", \"\", post)).strip().split()\n",
    "            for word in clean_words:\n",
    "                if word not in doc_freqs:\n",
    "                    doc_freqs[word] = [0 for x in xrange(nsubs)]\n",
    "                doc_freqs[word][i] = 1\n",
    "                freqs[word] += 1\n",
    "            lens.append(len(clean_words))\n",
    "            max_len = max(max_len, len(clean_words))\n",
    "\n",
    "doc_freqs = {word:sum(freq) for word, freq in doc_freqs.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Prune vocab\n",
    "    - break up into cells because each operation is pretty costly\n",
    "    \n",
    "'''\n",
    "\n",
    "min_doc_appearances = 1\n",
    "remove_top_k = 0 # TODO: maybe up this?\n",
    "max_vocab_size = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pruning vocab borrowed from Yoon Kim\n",
    "vocab = [(word, count) for word, count in freqs.iteritems()]\n",
    "vocab.sort(key = lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_common_words = [pair[0] for pair in vocab[remove_top_k:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_doc_words = filter(lambda x: doc_freqs[x] >= min_doc_appearances, no_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vocab...\n",
      "\tFull vocab size: 225198, pruned vocab size: 15000\n"
     ]
    }
   ],
   "source": [
    "if max_vocab_size <= 1:\n",
    "    stop_pt = int(max_vocab_size * len(min_doc_words))\n",
    "else:\n",
    "    stop_pt = max_vocab_size - len(special)\n",
    "pruned = min_doc_words[:stop_pt]\n",
    "\n",
    "word2ind = {}\n",
    "ind2word = {}\n",
    "ind = 1 # start with 1 for easy masking\n",
    "for word in special+pruned:\n",
    "    word2ind[word] = ind\n",
    "    ind2word[ind] = word\n",
    "    ind += 1\n",
    "\n",
    "print \"Writing vocab...\"\n",
    "with open(outfile+'.vocab.txt', 'w') as f:\n",
    "    f.write(\"Word Index Count DocFreq\\n\")\n",
    "    words = [(word, idx) for word, idx in word2ind.iteritems()]\n",
    "    words.sort(key = lambda x: x[1])\n",
    "    for word, idx in words:\n",
    "        if word in freqs:\n",
    "            f.write(\"%s %d %d %d\\n\" % (word, idx, freqs[word], doc_freqs[word]))\n",
    "        else:\n",
    "            f.write(\"%s %d %d %d\\n\" % (word, idx, -1, -1))\n",
    "\n",
    "with open(outfile+'.vocab.pkl', 'w') as f:\n",
    "    pickle.dump((word2ind, ind2word), f)\n",
    "\n",
    "print '\\tFull vocab size: %d, pruned vocab size: %d' % (len(vocab), len(word2ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Generate the data\n",
    "\n",
    "'''\n",
    "\n",
    "span_data = []\n",
    "#mask_data = [] # masks to be constructed in the train now \n",
    "sub_data = []\n",
    "user_data = []\n",
    "\n",
    "max_len = min(max_len, 116)\n",
    "\n",
    "lengths = []\n",
    "unks = []\n",
    "\n",
    "unk = word2ind['<UNK>']\n",
    "\n",
    "for i in xrange(n_subs):\n",
    "    #sub = np.array([i], dtype=np.int32)\n",
    "    for j in xrange(n_users):\n",
    "        if not posts[i][j]:\n",
    "            continue\n",
    "        #user = np.array([j], dtype=np.int32)\n",
    "        spans = []\n",
    "        masks = []\n",
    "        for post,_ in posts[i][j]:\n",
    "            clean_words = re.sub(r\"[0-9]{1,}\", \"<NUM>\", re.sub(r\"<(.+?)>\", \"\", post)).strip().split()\n",
    "            if not clean_words:\n",
    "                continue\n",
    "            span = [word2ind[word] if word in word2ind else unk for word in clean_words[:max_len]] + \\\n",
    "                    [0]*(max_len-len(clean_words))\n",
    "            sub_data.append(i)\n",
    "            user_data.append(j)\n",
    "            span_data.append(span)\n",
    "            \n",
    "            # analytics: number of unknowns and length of phrases\n",
    "            unks.append(sum(filter(lambda x: x == unk, span)))\n",
    "            lengths.append(len(span))\n",
    "\n",
    "sub_data = np.array(sub_data, dtype=np.int32)        \n",
    "user_data = np.array(user_data, dtype=np.int32)\n",
    "span_data = np.array(span_data, dtype=np.int32)\n",
    "unks = np.array(unks, dtype=np.float32) / unk\n",
    "lengths = np.array(lengths, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_data = np.array(sub_data, dtype=np.int32)        \n",
    "user_data = np.array(user_data, dtype=np.int32)\n",
    "span_data = np.array(span_data, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Write data to pickles and stuff\n",
    "\n",
    "'''\n",
    "metadata_path = '/dfs/scratch0/wangalex/rmn/reddit5_meta.pkl'\n",
    "span_path = '/dfs/scratch0/wangalex/rmn/reddit5_spans.hdf5'\n",
    "\n",
    "pickle.dump((word2ind, usr2ind, sub2ind), open(metadata_path, 'wb'))\n",
    "\n",
    "with h5py.File(span_path, \"w\") as f:\n",
    "    f['subs'] = sub_data\n",
    "    f['user'] = user_data\n",
    "    f['spans'] = span_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 15000 is out of bounds for axis 0 with size 15000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-176-f883bd94ea64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0membed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_vecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0membed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword2ind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"\\tLoaded %d vectors\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_vecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m#pickle.dump(embed, open(word2vec_path, 'wb'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 15000 is out of bounds for axis 0 with size 15000"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Build word2vec pretrained embeddings\n",
    "\n",
    "'''\n",
    "\n",
    "word_vecs = {}\n",
    "vec_file = '/dfs/scratch0/gigawordvecs/GoogleNews-vectors-negative300.bin'\n",
    "word2vec_path = '/dfs/scratch0/wangalex/rmn/glove.We'\n",
    "\n",
    "with open(vec_file, \"rb\") as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, layer1_size = map(int, header.split())\n",
    "    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "    for line in xrange(vocab_size):\n",
    "        word = []\n",
    "        while True:\n",
    "            ch = f.read(1)\n",
    "            if ch == ' ':\n",
    "                word = ''.join(word)\n",
    "                break\n",
    "            if ch != '\\n':\n",
    "                word.append(ch)\n",
    "        if word in word2ind:  \n",
    "            word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "        else:\n",
    "            f.read(binary_len)\n",
    "\n",
    "embed = np.random.uniform(-0.25, 0.25, (len(word2ind), len(word_vecs.values()[0])))\n",
    "embed[0] = 0\n",
    "for word, vec in word_vecs.items():\n",
    "    embed[word2ind[word]] = vec\n",
    "    \n",
    "print \"\\tLoaded %d vectors\" % len(word_vecs)\n",
    "with h5py.File('/dfs/scratch0/wangalex/rmn/w2v.hdf5', 'w') as f:\n",
    "    f['w2v'] = embed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
