{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "import re\n",
    "import pdb\n",
    "from collections import defaultdict, Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/dfs/scratch0/wleif/Reddit/clean_comments/'\n",
    "subreddits = os.listdir(path)\n",
    "subreddits = ['funny', 'pics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1\n",
      "Finished 2\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -n1 -r1\n",
    "print_every = 1\n",
    "\n",
    "usr2ind = {}\n",
    "ind2usr = {}\n",
    "n_users = 0\n",
    "\n",
    "sub2ind = {}\n",
    "ind2sub = {}\n",
    "n_subs = len(subreddits)\n",
    "\n",
    "users = {}\n",
    "\n",
    "for i, subreddit in enumerate(subreddits):\n",
    "    sub2ind[subreddit] = i\n",
    "    ind2sub[i] = subreddit\n",
    "    with open(path+subreddit+'.tsv', 'r') as df:\n",
    "        #raw_data = np.genfromtxt(df, dtype=str, delimiter='\\t')\n",
    "        for row in df:\n",
    "            user = row.split('\\t')[-5]\n",
    "            if user not in usr2ind:\n",
    "                ind2usr[n_users] = user\n",
    "                usr2ind[user] = n_users\n",
    "                users[n_users] = defaultdict(int)\n",
    "                n_users += 1\n",
    "            users[usr2ind[user]][i] += 1\n",
    "\n",
    "    \n",
    "    if i % print_every == 0:\n",
    "        print \"Finished %d\" % (i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsubs: 0, percent: 0.302, robust count: 586041, full count: 1939091\n",
      "nsubs: 1, percent: 0.125, robust count: 242105, full count: 898131\n",
      "nsubs: 2, percent: 0.059, robust count: 114459, full count: 433442\n",
      "nsubs: 3, percent: 0.007, robust count: 13687, full count: 67131\n",
      "nsubs: 4, percent: 0.000, robust count: 505, full count: 3684\n",
      "Total number of users: 1939091, Number of selected users: 13687\n",
      "Total number of posts: 40737131\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Some analytics\n",
    "\n",
    "'''\n",
    "\n",
    "counter = [0.0] * n_subs\n",
    "r_counter = [0.0] * n_subs\n",
    "n_posts = 0\n",
    "thresh = 2\n",
    "user_pool = []\n",
    "\n",
    "# TODO: document this shit\n",
    "def robust_count(d, n_spans = 5):\n",
    "    return sum([1 if v > n_spans else 0 for v in [v for v in d.values()] ])\n",
    "        \n",
    "for k, v in users.iteritems():\n",
    "    n_posts += sum([uv for _, uv in v.iteritems()])\n",
    "    for i in xrange(n_subs):\n",
    "        if len(v) > i:\n",
    "            counter[i] += 1\n",
    "        if robust_count(v) > i:\n",
    "            r_counter[i] += 1\n",
    "    if robust_count(v) > thresh:\n",
    "        user_pool.append(k)\n",
    "percents = [n / n_users for n in r_counter]\n",
    "for i, percent in enumerate(percents):\n",
    "    print \"nsubs: %d, percent: %.3f, robust count: %d, full count: %d\" % (i, percent, r_counter[i], counter[i])\n",
    "\n",
    "user_pool = set(user_pool)\n",
    "print \"Total number of users: %d, Number of selected users: %d\" % (n_users, len(user_pool))\n",
    "print \"Total number of posts: %d\" % n_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0.00\n",
      "Finished 1\n",
      "\t0.00\n",
      "Finished 2\n",
      "\t0.00\n",
      "Finished 3\n",
      "\t0.00\n",
      "Finished 4\n",
      "\t0.00\n",
      "Finished 5\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Get the posts for the users that selected population of users (user_pool)\n",
    "\n",
    "'''\n",
    "\n",
    "print_every = [1.,1.]\n",
    "posts = [[[] for x in xrange(n_users)] for y in xrange(n_subs)]\n",
    "\n",
    "try:\n",
    "    for i, subreddit in enumerate(subreddits):\n",
    "        nrows = sum(1 for row in open(path+subreddit+'.tsv', 'r'))\n",
    "        with open(path+subreddit+'.tsv', 'r') as df:\n",
    "            sub_ind = sub2ind[subreddit]\n",
    "            for j, row in enumerate(df):\n",
    "                data = row.split('\\t')\n",
    "                user = usr2ind[data[-5]]\n",
    "                if user in user_pool:\n",
    "                    posts[sub_ind][user].append((data[-1], data[1]))\n",
    "\n",
    "                if j % int(nrows/print_every[0]) == 0: ## TODO: Fix logging...\n",
    "                    print '\\t%.2f' % (j/float(nrows))\n",
    "\n",
    "        if i % print_every[1] == 0:\n",
    "            print \"Finished %d\" % (i+1)\n",
    "except Exception as e:\n",
    "    pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Build vocab\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "outfile = '/dfs/scratch0/wangalex/rmn/reddit5'\n",
    "\n",
    "freqs = defaultdict(int)\n",
    "doc_freqs = {}\n",
    "special = ['<EOS>', '<UNK>', '<SPECIAL>', '<URL>']\n",
    "lens = []\n",
    "max_len = 0\n",
    "\n",
    "print \"Sorting by date and gathering vocab...\"\n",
    "for i in xrange(n_subs):\n",
    "    for j in xrange(n_users):\n",
    "        if posts[i][j] is []:\n",
    "            continue\n",
    "        posts[i][j].sort(key=lambda tup:tup[-1])\n",
    "        for post,_ in posts[i][j]:\n",
    "            clean_words = re.sub(r\"[0-9]{1,}\", \"<NUM>\", re.sub(r\"<(.+?)>\", \"\", post)).strip().split()\n",
    "            for word in clean_words:\n",
    "                if word not in doc_freqs:\n",
    "                    doc_freqs[word] = [0 for x in xrange(nsubs)]\n",
    "                doc_freqs[word][i] = 1\n",
    "                freqs[word] += 1\n",
    "            lens.append(len(clean_words))\n",
    "            max_len = max(max_len, len(clean_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Prune vocab\n",
    "    - break up into cells because each operation is pretty costly\n",
    "    \n",
    "'''\n",
    "\n",
    "min_doc_appearances = 4\n",
    "remove_top_k = 500 # TODO: maybe up this?\n",
    "max_vocab_size = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pruning vocab borrowed from Yoon Kim\n",
    "vocab = [(word, count) for word, count in freqs.iteritems()]\n",
    "vocab.sort(key = lambda x: x[1], reverse = True)\n",
    "doc_freqs = {word:sum(freq) for word, freq in doc_freqs.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_common_words = [pair[0] for pair in vocab[remove_top_k:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_doc_words = filter(lambda x: doc_freqs[x] >= min_doc_appearances, no_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vocab...\n",
      "\tFull vocab size: 519523, pruned vocab size: 29821\n"
     ]
    }
   ],
   "source": [
    "if max_vocab_size <= 1:\n",
    "    stop_pt = int(max_vocab_size * len(min_doc_words))\n",
    "else:\n",
    "    stop_pt = max_vocab_size - len(special)\n",
    "pruned = min_doc_words[:stop_pt]\n",
    "\n",
    "word2ind = {}\n",
    "ind2word = {}\n",
    "ind = 1 # start with 1 for easy masking\n",
    "for word in special+pruned:\n",
    "    word2ind[word] = ind\n",
    "    ind2word[ind] = word\n",
    "    ind += 1\n",
    "\n",
    "print \"Writing vocab...\"\n",
    "with open(outfile+'.vocab.txt', 'w') as f:\n",
    "    f.write(\"Word Index Count DocFreq\\n\")\n",
    "    words = [(word, idx) for word, idx in word2ind.iteritems()]\n",
    "    words.sort(key = lambda x: x[1])\n",
    "    for word, idx in words:\n",
    "        if word in freqs:\n",
    "            f.write(\"%s %d %d %d\\n\" % (word, idx, freqs[word], doc_freqs[word]))\n",
    "        else:\n",
    "            f.write(\"%s %d %d %d\\n\" % (word, idx, -1, -1))\n",
    "\n",
    "with open(outfile+'.vocab.pkl', 'w') as f:\n",
    "    pickle.dump((word2ind, ind2word), f)\n",
    "\n",
    "print '\\tFull vocab size: %d, pruned vocab size: %d' % (len(vocab), len(word2ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Generate the data\n",
    "\n",
    "'''\n",
    "\n",
    "span_data = []\n",
    "#mask_data = [] # masks to be constructed in the train now \n",
    "sub_data = []\n",
    "user_data = []\n",
    "\n",
    "max_len = min(max_len, 116)\n",
    "\n",
    "lengths = []\n",
    "unks = []\n",
    "\n",
    "unk = word2ind['<UNK>']\n",
    "\n",
    "for i in xrange(n_subs):\n",
    "    #sub = np.array([i], dtype=np.int32)\n",
    "    for j in xrange(n_users):\n",
    "        if not posts[i][j]:\n",
    "            continue\n",
    "        #user = np.array([j], dtype=np.int32)\n",
    "        spans = []\n",
    "        masks = []\n",
    "        for post,_ in posts[i][j]:\n",
    "            clean_words = re.sub(r\"[0-9]{1,}\", \"<NUM>\", re.sub(r\"<(.+?)>\", \"\", post)).strip().split()\n",
    "            if not clean_words:\n",
    "                continue\n",
    "            span = [word2ind[word] if word in word2ind else unk for word in clean_words[:max_len]]\n",
    "            \n",
    "            # analytics: number of unknowns and length of phrases\n",
    "            unks.append(sum(filter(lambda x: x == unk, span)))\n",
    "            lengths.append(len(span))\n",
    "            \n",
    "            #mask = [1] * len(span) + [0] * (max_len-len(clean_words))\n",
    "            span += [0]*(max_len-len(clean_words))\n",
    "            #spans.append(span)\n",
    "            #masks.append(mask)\n",
    "            sub_data.append(i)\n",
    "            user_data.append(j)\n",
    "            span_data.append(span)\n",
    "            #mask_data.append(masks)\n",
    "\n",
    "'''\n",
    "sub_data = np.array(sub_data, dtype=np.int32)        \n",
    "user_data = np.array(user_data, dtype=np.int32)\n",
    "span_data = np.array(span_data, dtype=np.int32)\n",
    "mask_data = np.array(mask_data, dtype=np.int32)\n",
    "'''\n",
    "\n",
    "unks = np.array(unks, dtype=np.float32) / unk\n",
    "lengths = np.array(lengths, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71856135"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(unks/lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nf = h5py.File(span_path, \"w\")\\nf[\\'span_data\\'] = span_data\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Write data to pickles and stuff\n",
    "\n",
    "'''\n",
    "metadata_path = '/dfs/scratch0/wangalex/rmn/reddit5_meta.pkl'\n",
    "span_path = '/dfs/scratch0/wangalex/rmn/reddit5_spans.hdf5'\n",
    "pickle.dump((word2ind, usr2ind, sub2ind), open(metadata_path, 'wb'))\n",
    "#pickle.dump(span_data, open(span_path, 'wb'))\n",
    "\n",
    "f = h5py.File(span_path, \"w\")\n",
    "f['subs'] = sub_data\n",
    "f['user'] = user_data\n",
    "f['spans'] = span_data\n",
    "#f['masks'] = mask_data # can save space by doing this in train function\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoaded 9601 vectors\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Build word2vec pretrained embeddings\n",
    "\n",
    "'''\n",
    "\n",
    "word_vecs = {}\n",
    "vec_file = '/dfs/scratch0/gigawordvecs/GoogleNews-vectors-negative300.bin'\n",
    "word2vec_path = '/dfs/scratch0/wangalex/rmn/glove.We'\n",
    "\n",
    "with open(vec_file, \"rb\") as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, layer1_size = map(int, header.split())\n",
    "    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "    for line in xrange(vocab_size):\n",
    "        word = []\n",
    "        while True:\n",
    "            ch = f.read(1)\n",
    "            if ch == ' ':\n",
    "                word = ''.join(word)\n",
    "                break\n",
    "            if ch != '\\n':\n",
    "                word.append(ch)\n",
    "        if word in word2ind:  \n",
    "            word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "        else:\n",
    "            f.read(binary_len)\n",
    "\n",
    "embed = np.random.uniform(-0.25, 0.25, (len(vocab), len(word_vecs.values()[0])))\n",
    "embed[0] = 0\n",
    "for word, vec in word_vecs.items():\n",
    "    embed[word2ind[word]] = vec\n",
    "print \"\\tLoaded %d vectors\" % len(word_vecs)\n",
    "#pickle.dump(embed, open(word2vec_path, 'wb'))\n",
    "f = h5py.File('/dfs/scratch0/wangalex/rmn/w2v.hdf5', 'w')\n",
    "f['w2v'] = embed\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedforward  ops.py\trmn  subreddits.ipynb\t    subreddits.txt\r\n",
      "main.py      README.md\trnn  subreddits_sorted.txt\r\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
